{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851e007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from models import BalmEmbedder, Ablang2Embedder, Esm2Embedder, AntibertyEmbedder, AblangHeavyEmbedder, IgbertEmbedder\n",
    "from time import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86b551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3041, 24) (1909, 42)\n",
      "['Unnamed: 0', 'NAME', 'EPITOPE', 'HV', 'HJ', 'LV', 'LJ', 'HC_AA', 'LC_AA', 'DONOR', 'CDRH3', 'CDRL3', 'EPITOPE_REGION', 'AG_BROAD', 'AG_NARROW', 'EMBEDDING_IDX', 'CLONOTYPE', 'DATASET', 'CLASS', 'INDEX', 'PREPARED_HC_SEQ', 'PREPARED_LC_SEQ', 'EPITOPE_LABELS', 'SYNTHETIC'] ['Column1', 'AG_CLUSTER', 'AG_AA', 'CLAN', 'PFAM_PLUS', 'NAME_x', 'HV', 'HJ', 'CDRH3', 'HC_AA', 'LV', 'LJ', 'CDRL3', 'LC_AA', 'CLONE', 'NUM_TRAIN_PAIRS', 'NUM_TRAIN_SAME_PFAM_DIF_EP', 'NUM_TRAIN_OVERLAPPING_EP', 'DATASET', 'PREPARED_HC_SEQ', 'PREPARED_LC_SEQ', 'H_TOKENS', 'L_TOKENS', 'H_ATTENTION_MASK', 'L_ATTENTION_MASK', 'SAME_AG_POSSIBLE', 'SAME_EP_POSSIBLE', 'NN_ACTUAL', 'NN_ACTUAL_PFAM', 'NN_ACTUAL_COS_SIM', 'NN_ACTUAL_PREDICTED_COS_SIM', 'NN_ACTUAL_PREDICTED_RANK', 'NN_GUESS', 'NN_GUESS_PFAM', 'NN_GUESS_COS_SIM', 'NN_GUESS_ACTUAL_LABEL', 'NN_GUESS_RANK', 'CORRECT_NN_EP_GUESS', 'CORRECT_NN_PFAM_GUESS', 'PLOTTING_AG', 't-SNE_1', 't-SNE_2']\n"
     ]
    }
   ],
   "source": [
    "sabdab_df = pd.read_parquet('ablangpdb_renameddatasets.parquet').drop(columns=\"EMBEDDING\")\n",
    "dms_df = pd.read_parquet('ablangrbd_renameddatasets.parquet').drop(columns=\"EMBEDDING\")\n",
    "print(dms_df.shape, sabdab_df.shape)\n",
    "print(dms_df.columns.to_list(), sabdab_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5a6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "balm_model_dir = \"../../../pretrained_models/BALM-paired_LC-coherence_90-5-5-split_122222\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70651274",
   "metadata": {},
   "source": [
    "\n",
    "            if model_name == \"BALM\":\n",
    "1&2: Fail. Path error\n",
    "            elif model_name == \"AbLang2\":\n",
    "1&2: Fail.    model.eval() call\n",
    "            elif model_name == \"ESM-2\":\n",
    "1 Success\n",
    "2. Fail. Cuda\n",
    "            elif model_name == \"AntiBERTy\":\n",
    "1 Success\n",
    "2. Fail. Cuda\n",
    "            elif model_name == \"AbLang-Heavy\":\n",
    "1 Fail. Cuda\n",
    "2. Fail. Cuda\n",
    "            else:  # model_name == \"IgBERT\":\n",
    "1. Fail. Cuda\n",
    "2 Fail. Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac0ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping sabdab_embeddedby_balm.parquet\n",
      "Skipping sabdab_embeddedby_ablang2.parquet\n",
      "Skipping sabdab_embeddedby_esm-2.parquet\n",
      "Skipping sabdab_embeddedby_antiberty.parquet\n",
      "Skipping sabdab_embeddedby_ablang-heavy.parquet\n",
      "Skipping sabdab_embeddedby_igbert.parquet\n",
      "Skipping dms_embeddedby_balm.parquet\n",
      "Skipping dms_embeddedby_ablang2.parquet\n",
      "Skipping dms_embeddedby_esm-2.parquet\n",
      "Starting with model  AntiBERTy\n",
      "Embedder set up, going on to embed now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:14<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embeddings to dms_embeddedby_antiberty.parquet...\n",
      "Suceeded with model AntiBERTy Shape (3041, 25)\n",
      "\n",
      "\n",
      "Skipping dms_embeddedby_ablang-heavy.parquet\n",
      "Skipping dms_embeddedby_igbert.parquet\n"
     ]
    }
   ],
   "source": [
    "# 2. Generate Embeddings\n",
    "skip_existing_embeds = True\n",
    "datasets = [(\"SABDAB\", sabdab_df), (\"DMS\", dms_df)]\n",
    "models = [\"BALM\", \"AbLang2\", \"ESM-2\", \"AntiBERTy\", \"AbLang-Heavy\", \"IgBERT\"]\n",
    "# datasets = [(\"SABDAB\", sabdab_df)]\n",
    "# models = [\"AbLang2\", \"AbLang-Heavy\"] #, \"IgBERT\"]\n",
    "for dataset, starting_df in datasets:  # for 2 datasets\n",
    "    for model_name in models:  # Use 6 models for embedding\n",
    "        output_file = f\"{dataset.lower()}_embeddedby_{model_name.lower()}.parquet\"\n",
    "        if os.path.exists(output_file) and skip_existing_embeds:  # Only do it if file doesn't exist\n",
    "            print(\"Skipping\", output_file)\n",
    "            continue\n",
    "        \n",
    "        t1 = time()\n",
    "        try:\n",
    "            print(\"Starting with model \", model_name)\n",
    "            \n",
    "            df = starting_df.copy() # Make sure not to overwrite original\n",
    "\n",
    "            if model_name == \"BALM\":\n",
    "                embedder = BalmEmbedder(model_directory=balm_model_dir, device=device)\n",
    "            elif model_name == \"AbLang2\":\n",
    "                embedder = Ablang2Embedder(device=device)\n",
    "            elif model_name == \"ESM-2\":\n",
    "                embedder = Esm2Embedder(device=device)\n",
    "            elif model_name == \"AntiBERTy\":\n",
    "                embedder = AntibertyEmbedder()\n",
    "            elif model_name == \"AbLang-Heavy\":\n",
    "                embedder = AblangHeavyEmbedder(device=device)\n",
    "            else:  # model_name == \"IgBERT\":\n",
    "                embedder = IgbertEmbedder(device=device)\n",
    "\n",
    "            print(\"Embedder set up, going on to embed now\")\n",
    "            embeddings = embedder.embed(df)\n",
    "            df[\"EMBEDDING\"] = list(embeddings)\n",
    "            del embeddings, embedder  # Clean up memory\n",
    "\n",
    "            # 3. Save Data\n",
    "            print(f\"Saving embeddings to {output_file}...\")\n",
    "            df.to_parquet(output_file)\n",
    "            print(\"Suceeded with model\", model_name, f\"Shape {df.shape}\\n\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model {model_name} failed with exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6a2df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sabdab_embeddedby_ablang-heavy.parquet (1909, 43)\n",
      "sabdab_embeddedby_ablang2.parquet (1909, 43)\n",
      "sabdab_embeddedby_ablangpre.parquet (1909, 43)\n",
      "sabdab_embeddedby_ablangrbd.parquet (1909, 43)\n",
      "sabdab_embeddedby_antiberty.parquet (1909, 43)\n",
      "sabdab_embeddedby_balm.parquet (1909, 43)\n",
      "sabdab_embeddedby_esm-2.parquet (1909, 43)\n",
      "sabdab_embeddedby_igbert.parquet (1909, 43)\n",
      "sabdab_embeddedby_parapred.parquet (1909, 48)\n",
      "dms_embeddedby_ablang-heavy.parquet (3041, 25)\n",
      "dms_embeddedby_ablang2.parquet (3041, 25)\n",
      "dms_embeddedby_ablangpdb.parquet (3041, 25)\n",
      "dms_embeddedby_ablangpre.parquet (3041, 25)\n",
      "dms_embeddedby_antiberty.parquet (3041, 25)\n",
      "dms_embeddedby_balm.parquet (3041, 25)\n",
      "dms_embeddedby_esm-2.parquet (3041, 25)\n",
      "dms_embeddedby_igbert.parquet (3041, 25)\n",
      "dms_embeddedby_parapred.parquet (3041, 30)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "for x in [\"sabdab\", \"dms\"]:\n",
    "    for fname in glob(f\"{x}*.parquet\"):\n",
    "        df = pd.read_parquet(fname)\n",
    "        print(fname, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f691c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 20197.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n",
      "['c', 'd']\n",
      "['E']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "batch_size = 2\n",
    "sequences = [\"a\", \"b\", \"c\", \"d\", \"E\"]\n",
    "for i in tqdm(range(0, len(sequences),batch_size)):\n",
    "        batch_sequences = sequences[i:i+batch_size]\n",
    "        print(batch_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\"BALM\", \"AbLang2\", \"ESM-2\", \"AntiBERTy\", \"AbLang-Heavy\", \"IgBERT\"]\n",
    "failed = [\"BALM\", \"AbLang2\"]\n",
    "succeeded = [\"?ESM-2\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pub_clone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
