{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test vs Test Benchmarking Pipeline with Excel Export\n",
    "\n",
    "This notebook provides a complete workflow for:\n",
    "1. Running comprehensive benchmarks across multiple models and datasets using **TEST vs TEST** comparisons\n",
    "2. Using **VAL vs VAL** comparisons for F1 threshold optimization\n",
    "3. Generating standardized summary files\n",
    "4. Creating formatted Excel reports with performance rankings\n",
    "\n",
    "**Models included:**\n",
    "- **AbLang Family**: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy (cosine similarity)\n",
    "- **Other Protein LMs**: AntiBERTy, BALM, ESM-2, IgBERT, Parapred (cosine similarity)\n",
    "- **Sequence-based**: SEQID (sequence identity), CDRH3ID (CDRH3 identity)\n",
    "- **Note**: ABodyBuilder2 DTW configurations are excluded from this analysis\n",
    "\n",
    "**Datasets:**\n",
    "- SAbDab (structural antibody database)\n",
    "- DMS (deep mutational scanning)\n",
    "\n",
    "**Key Differences from Train vs Test Analysis:**\n",
    "- Compares test antibodies against other test antibodies\n",
    "- Uses validation set for threshold optimization instead of training set\n",
    "- Uses test vs test label matrices and val vs val label matrices\n",
    "- Excludes ABodyBuilder2 structural similarity calculations\n",
    "\n",
    "**Features:**\n",
    "- Works with ALL available parquet files in the benchmarking directory\n",
    "- Optional embedding re-calculation toggle (disabled by default)\n",
    "- Optional threshold reuse for faster re-runs\n",
    "- Comprehensive error handling and logging\n",
    "- Supports 12 different models across both datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  ‚Ä¢ Recalculate embeddings: False\n",
      "  ‚Ä¢ Recalculate summary metrics: False\n",
      "  ‚Ä¢ Output folder: output_csvs_test\n",
      "  ‚Ä¢ Excel filename: comprehensive_benchmarking_results_test.xlsx\n",
      "  ‚Ä¢ Batch size: 256\n",
      "\n",
      "üìù Note: This notebook supports test vs test comparisons:\n",
      "  ‚Ä¢ Models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy,\n",
      "           AntiBERTy, BALM, ESM-2, IgBERT, Parapred, SEQID, CDRH3ID\n",
      "  ‚Ä¢ Datasets: SAbDab and DMS\n",
      "  ‚Ä¢ Comparison type: TEST vs TEST (thresholded using VAL vs VAL)\n",
      "  ‚Ä¢ ABodyBuilder2 DTW configurations excluded\n",
      "  ‚Ä¢ Only existing parquet files will be processed\n",
      "  ‚Ä¢ Summary metrics will be reused if available\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION FLAGS - MODIFY THESE AS NEEDED\n",
    "# =============================================================================\n",
    "\n",
    "# Set to False to use existing parquet files (faster), True to re-calculate embeddings\n",
    "# NOTE: This notebook now works with ALL available parquet files in the directory\n",
    "RECALCULATE_EMBEDDINGS = False\n",
    "\n",
    "# Set to False to skip benchmark recalculation if summary files exist (faster), True to always recalculate\n",
    "RECALCULATE_SUMMARYMETRICS = False\n",
    "\n",
    "# Model paths - update these paths as needed\n",
    "MODEL_PATHS = {\n",
    "    \"AbLangPDB\": \"../../../huggingface/AbLangPDB1/ablangpdb_model.safetensors\",\n",
    "    \"AbLangRBD\": \"../../../huggingface/AbLangRBD1/model.safetensors\"\n",
    "}\n",
    "\n",
    "# Batch size for embedding generation\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_FOLDER = \"output_csvs_test\"\n",
    "EXCEL_FILENAME = \"comprehensive_benchmarking_results_test.xlsx\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  ‚Ä¢ Recalculate embeddings: {RECALCULATE_EMBEDDINGS}\")\n",
    "print(f\"  ‚Ä¢ Recalculate summary metrics: {RECALCULATE_SUMMARYMETRICS}\")\n",
    "print(f\"  ‚Ä¢ Output folder: {OUTPUT_FOLDER}\")\n",
    "print(f\"  ‚Ä¢ Excel filename: {EXCEL_FILENAME}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nüìù Note: This notebook supports test vs test comparisons:\")\n",
    "print(f\"  ‚Ä¢ Models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2, AbLang-Heavy,\")\n",
    "print(f\"           AntiBERTy, BALM, ESM-2, IgBERT, Parapred, SEQID, CDRH3ID\")\n",
    "print(f\"  ‚Ä¢ Datasets: SAbDab and DMS\")\n",
    "print(f\"  ‚Ä¢ Comparison type: TEST vs TEST (thresholded using VAL vs VAL)\")\n",
    "print(f\"  ‚Ä¢ ABodyBuilder2 DTW configurations excluded\")\n",
    "print(f\"  ‚Ä¢ Only existing parquet files will be processed\")\n",
    "print(f\"  ‚Ä¢ Summary metrics will be {'recalculated' if RECALCULATE_SUMMARYMETRICS else 'reused if available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import typing as T\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import local modules\n",
    "import calculate_metrics\n",
    "import calculate_metrics_dms\n",
    "import models\n",
    "from excel_generator import generate_results_excel, print_summary_stats\n",
    "from ablangpaired_model import AbLangPairedConfig, AbLangPaired\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def check_file_exists(filepath, description=\"\"):\n",
    "    \"\"\"Check if a file exists and return status.\"\"\"\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {description}: {filepath}\")\n",
    "    return exists\n",
    "\n",
    "\n",
    "def embed_with_ablangpaired(input_path: str, output_path: str, model_path: str, model_name: str):\n",
    "    \"\"\"Generate embeddings using AbLangPaired models.\n",
    "    \n",
    "        Args:\n",
    "            model_name: str. If \"ablangpre\" then the model architecture will no longer have the mixer layer.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîÑ Generating {model_name} embeddings...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_parquet(input_path)\n",
    "    if \"EMBEDDING\" in df.columns:\n",
    "        df = df.drop(columns=[\"EMBEDDING\"])\n",
    "    \n",
    "    # Setup model\n",
    "    model_config = AbLangPairedConfig(checkpoint_filename=model_path)\n",
    "    is_ablangpre = model_name == \"ablangpre\"\n",
    "    model = AbLangPaired(model_config, device=device, use_pretrained=is_ablangpre)\n",
    "    \n",
    "    # Tokenize and embed using enhanced methods\n",
    "    tokenized_dataloader = models.tokenize_data(df, model_config, batch_size=BATCH_SIZE)\n",
    "    all_embeds = models.embed_dataloader(tokenized_dataloader, model, device)\n",
    "    \n",
    "    # Save\n",
    "    df['EMBEDDING'] = list(all_embeds.cpu().numpy())\n",
    "    df.to_parquet(output_path)\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} embeddings saved to {output_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_summary_file_paths(model_name: str, dataset_type: str, output_folder: str):\n",
    "    \"\"\"Get expected summary file paths for a model/dataset combination.\"\"\"\n",
    "    if dataset_type == \"sabdab\":\n",
    "        # SAbDab has both epitope and antigen summary files\n",
    "        epitope_file = os.path.join(output_folder, f\"{model_name}_sabdab_ep_summarymetrics.txt\")\n",
    "        antigen_file = os.path.join(output_folder, f\"{model_name}_sabdab_ag_summarymetrics.txt\")\n",
    "        return [epitope_file, antigen_file]\n",
    "    elif dataset_type == \"dms\":\n",
    "        # DMS has only one summary file\n",
    "        dms_file = os.path.join(output_folder, f\"{model_name}_dms_summarymetrics.txt\")\n",
    "        return [dms_file]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def read_and_display_summary_results(summary_files: list, model_name: str, dataset_type: str):\n",
    "    \"\"\"Read and display results from existing summary files.\"\"\"\n",
    "    print(f\"\\nüìä Loading existing results for {model_name} on {dataset_type}:\")\n",
    "    \n",
    "    for summary_file in summary_files:\n",
    "        if os.path.exists(summary_file):\n",
    "            try:\n",
    "                with open(summary_file, 'r') as f:\n",
    "                    content = f.read().strip()\n",
    "                \n",
    "                # Extract the task type from filename\n",
    "                if \"sabdab_ep\" in summary_file:\n",
    "                    task_type = \"Epitope-level performance\"\n",
    "                elif \"sabdab_ag\" in summary_file:\n",
    "                    task_type = \"Antigen-level performance\"\n",
    "                else:\n",
    "                    task_type = \"Performance\"\n",
    "                \n",
    "                print(f\"\\n--- {task_type} ---\")\n",
    "                print(content)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error reading {summary_file}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Summary file not found: {summary_file}\")\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Required Base Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Checking base dataset files...\n",
      "  ‚úÖ SAbDab base dataset: ablangpdb_renameddatasets.parquet\n",
      "  ‚úÖ DMS base dataset: ablangrbd_renameddatasets.parquet\n",
      "  ‚úÖ SAbDab validation labels (test vs test): ablangpdb_val_label_mat.pt\n",
      "  ‚úÖ SAbDab test labels (test vs test): ablangpdb_test_label_mat.pt\n",
      "  ‚úÖ DMS validation labels (test vs test): dms_val_label_mat.pt\n",
      "  ‚úÖ DMS test labels (test vs test): dms_test_label_mat.pt\n",
      "\n",
      "‚úÖ All base files found!\n"
     ]
    }
   ],
   "source": [
    "# Check that base dataset files exist\n",
    "print(\"üìã Checking base dataset files...\")\n",
    "\n",
    "base_files = {\n",
    "    \"SAbDab base dataset\": \"ablangpdb_renameddatasets.parquet\",\n",
    "    \"DMS base dataset\": \"ablangrbd_renameddatasets.parquet\",\n",
    "    \"SAbDab validation labels (test vs test)\": \"ablangpdb_val_label_mat.pt\",\n",
    "    \"SAbDab test labels (test vs test)\": \"ablangpdb_test_label_mat.pt\",\n",
    "    \"DMS validation labels (test vs test)\": \"dms_val_label_mat.pt\",\n",
    "    \"DMS test labels (test vs test)\": \"dms_test_label_mat.pt\"\n",
    "}\n",
    "\n",
    "missing_base_files = []\n",
    "for desc, filepath in base_files.items():\n",
    "    if not check_file_exists(filepath, desc):\n",
    "        missing_base_files.append(filepath)\n",
    "\n",
    "if missing_base_files:\n",
    "    raise FileNotFoundError(f\"Missing required base files: {missing_base_files}\")\n",
    "\n",
    "print(\"\\n‚úÖ All base files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EMBEDDING FILE STATUS CHECK\n",
      "============================================================\n",
      " Checking for existing embedding files (no regeneration)...\n",
      "‚úÖ Found: sabdab_embeddedby_ablangrbd.parquet\n",
      "‚úÖ Found: sabdab_embeddedby_ablangpre.parquet\n",
      "‚úÖ Found: sabdab_embeddedby_ablang2.parquet\n",
      "‚úÖ Found: sabdab_embeddedby_ablang-heavy.parquet\n",
      "‚úÖ Found: sabdab_embeddedby_antiberty.parquet\n",
      "‚úÖ Found: sabdab_embeddedby_balm.parquet\n",
      "‚úÖ Found: sabdab_embeddedby_esm-2.parquet\n",
      "‚úÖ Found: sabdab_embeddedby_igbert.parquet\n",
      "‚úÖ Found: sabdab_embeddedby_parapred.parquet\n",
      "‚úÖ Found: dms_embeddedby_ablangpdb.parquet\n",
      "‚úÖ Found: dms_embeddedby_ablangpre.parquet\n",
      "‚úÖ Found: dms_embeddedby_ablang2.parquet\n",
      "‚úÖ Found: dms_embeddedby_ablang-heavy.parquet\n",
      "‚úÖ Found: dms_embeddedby_antiberty.parquet\n",
      "‚úÖ Found: dms_embeddedby_balm.parquet\n",
      "‚úÖ Found: dms_embeddedby_esm-2.parquet\n",
      "‚úÖ Found: dms_embeddedby_igbert.parquet\n",
      "‚úÖ Found: dms_embeddedby_parapred.parquet\n",
      "\n",
      "üìä Embedding Files Summary:\n",
      "  ‚Ä¢ Existing files: 18/18\n",
      "  ‚Ä¢ Missing files: 0\n",
      "\n",
      "‚úÖ Embedding file check complete!\n"
     ]
    }
   ],
   "source": [
    "# Define all embedding files that should exist\n",
    "embedding_files = {\n",
    "    # SAbDab dataset embeddings\n",
    "    \"sabdab_embeddedby_ablangrbd.parquet\": (\"ablangpdb_renameddatasets.parquet\", MODEL_PATHS.get(\"AbLangRBD\"), \"AbLangRBD\"),\n",
    "    \"sabdab_embeddedby_ablangpre.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AbLangPre\"),\n",
    "    \"sabdab_embeddedby_ablang2.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AbLang2\"),\n",
    "    \"sabdab_embeddedby_ablang-heavy.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AbLang-Heavy\"),\n",
    "    \"sabdab_embeddedby_antiberty.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"AntiBERTy\"),\n",
    "    \"sabdab_embeddedby_balm.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"BALM\"),\n",
    "    \"sabdab_embeddedby_esm-2.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"ESM-2\"),\n",
    "    \"sabdab_embeddedby_igbert.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"IgBERT\"),\n",
    "    \"sabdab_embeddedby_parapred.parquet\": (\"ablangpdb_renameddatasets.parquet\", None, \"Parapred\"),\n",
    "\n",
    "    # DMS dataset embeddings\n",
    "    \"dms_embeddedby_ablangpdb.parquet\": (\"ablangrbd_renameddatasets.parquet\", MODEL_PATHS.get(\"AbLangPDB\"), \"AbLangPDB\"),\n",
    "    \"dms_embeddedby_ablangpre.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AbLangPre\"),\n",
    "    \"dms_embeddedby_ablang2.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AbLang2\"),\n",
    "    \"dms_embeddedby_ablang-heavy.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AbLang-Heavy\"),\n",
    "    \"dms_embeddedby_antiberty.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"AntiBERTy\"),\n",
    "    \"dms_embeddedby_balm.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"BALM\"),\n",
    "    \"dms_embeddedby_esm-2.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"ESM-2\"),\n",
    "    \"dms_embeddedby_igbert.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"IgBERT\"),\n",
    "    \"dms_embeddedby_parapred.parquet\": (\"ablangrbd_renameddatasets.parquet\", None, \"Parapred\")\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EMBEDDING FILE STATUS CHECK\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\" Checking for existing embedding files (no regeneration)...\")\n",
    "\n",
    "existing_files = []\n",
    "missing_files = []\n",
    "\n",
    "for output_file, (input_file, model_path, model_name) in embedding_files.items():\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"‚úÖ Found: {output_file}\")\n",
    "        existing_files.append(output_file)\n",
    "    else:\n",
    "        print(f\"‚ùå Missing: {output_file}\")\n",
    "        missing_files.append(output_file)\n",
    "\n",
    "print(f\"\\nüìä Embedding Files Summary:\")\n",
    "print(f\"  ‚Ä¢ Existing files: {len(existing_files)}/{len(embedding_files)}\")\n",
    "print(f\"  ‚Ä¢ Missing files: {len(missing_files)}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing embedding files:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"  - {file}\")\n",
    "    print(\"\\nNote: Configurations using missing files will be skipped automatically.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Embedding file check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify All Required Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Verifying all required files...\n",
      "  ‚úÖ SAbDab base (AbLangPDB embeddings): ablangpdb_renameddatasets.parquet\n",
      "  ‚úÖ DMS base: ablangrbd_renameddatasets.parquet\n",
      "  ‚úÖ SAbDab + AbLangRBD: sabdab_embeddedby_ablangrbd.parquet\n",
      "  ‚úÖ SAbDab + AbLangPre: sabdab_embeddedby_ablangpre.parquet\n",
      "  ‚úÖ SAbDab + AbLang2: sabdab_embeddedby_ablang2.parquet\n",
      "  ‚úÖ SAbDab + AbLang-Heavy: sabdab_embeddedby_ablang-heavy.parquet\n",
      "  ‚úÖ SAbDab + AntiBERTy: sabdab_embeddedby_antiberty.parquet\n",
      "  ‚úÖ SAbDab + BALM: sabdab_embeddedby_balm.parquet\n",
      "  ‚úÖ SAbDab + ESM-2: sabdab_embeddedby_esm-2.parquet\n",
      "  ‚úÖ SAbDab + IgBERT: sabdab_embeddedby_igbert.parquet\n",
      "  ‚úÖ SAbDab + Parapred: sabdab_embeddedby_parapred.parquet\n",
      "  ‚úÖ DMS + AbLangPDB: dms_embeddedby_ablangpdb.parquet\n",
      "  ‚úÖ DMS + AbLangPre: dms_embeddedby_ablangpre.parquet\n",
      "  ‚úÖ DMS + AbLang2: dms_embeddedby_ablang2.parquet\n",
      "  ‚úÖ DMS + AbLang-Heavy: dms_embeddedby_ablang-heavy.parquet\n",
      "  ‚úÖ DMS + AntiBERTy: dms_embeddedby_antiberty.parquet\n",
      "  ‚úÖ DMS + BALM: dms_embeddedby_balm.parquet\n",
      "  ‚úÖ DMS + ESM-2: dms_embeddedby_esm-2.parquet\n",
      "  ‚úÖ DMS + IgBERT: dms_embeddedby_igbert.parquet\n",
      "  ‚úÖ DMS + Parapred: dms_embeddedby_parapred.parquet\n",
      "  ‚úÖ SAbDab validation labels (test vs test): ablangpdb_val_label_mat.pt\n",
      "  ‚úÖ SAbDab test labels (test vs test): ablangpdb_test_label_mat.pt\n",
      "  ‚úÖ DMS validation labels (test vs test): dms_val_label_mat.pt\n",
      "  ‚úÖ DMS test labels (test vs test): dms_test_label_mat.pt\n",
      "\n",
      "‚úÖ All required files are available!\n"
     ]
    }
   ],
   "source": [
    "# Check that all required files now exist\n",
    "print(\"\\nüìã Verifying all required files...\")\n",
    "\n",
    "all_required_files = {\n",
    "    # Base datasets\n",
    "    \"SAbDab base (AbLangPDB embeddings)\": \"ablangpdb_renameddatasets.parquet\",\n",
    "    \"DMS base\": \"ablangrbd_renameddatasets.parquet\",\n",
    "    \n",
    "    # SAbDab embedding files\n",
    "    \"SAbDab + AbLangRBD\": \"sabdab_embeddedby_ablangrbd.parquet\",\n",
    "    \"SAbDab + AbLangPre\": \"sabdab_embeddedby_ablangpre.parquet\",\n",
    "    \"SAbDab + AbLang2\": \"sabdab_embeddedby_ablang2.parquet\",\n",
    "    \"SAbDab + AbLang-Heavy\": \"sabdab_embeddedby_ablang-heavy.parquet\",\n",
    "    \"SAbDab + AntiBERTy\": \"sabdab_embeddedby_antiberty.parquet\",\n",
    "    \"SAbDab + BALM\": \"sabdab_embeddedby_balm.parquet\",\n",
    "    \"SAbDab + ESM-2\": \"sabdab_embeddedby_esm-2.parquet\",\n",
    "    \"SAbDab + IgBERT\": \"sabdab_embeddedby_igbert.parquet\",\n",
    "    \"SAbDab + Parapred\": \"sabdab_embeddedby_parapred.parquet\",\n",
    "    \n",
    "    # DMS embedding files\n",
    "    \"DMS + AbLangPDB\": \"dms_embeddedby_ablangpdb.parquet\",\n",
    "    \"DMS + AbLangPre\": \"dms_embeddedby_ablangpre.parquet\",\n",
    "    \"DMS + AbLang2\": \"dms_embeddedby_ablang2.parquet\",\n",
    "    \"DMS + AbLang-Heavy\": \"dms_embeddedby_ablang-heavy.parquet\",\n",
    "    \"DMS + AntiBERTy\": \"dms_embeddedby_antiberty.parquet\",\n",
    "    \"DMS + BALM\": \"dms_embeddedby_balm.parquet\",\n",
    "    \"DMS + ESM-2\": \"dms_embeddedby_esm-2.parquet\",\n",
    "    \"DMS + IgBERT\": \"dms_embeddedby_igbert.parquet\",\n",
    "    \"DMS + Parapred\": \"dms_embeddedby_parapred.parquet\",\n",
    "    \n",
    "    # Label matrices (test vs test)\n",
    "    \"SAbDab validation labels (test vs test)\": \"ablangpdb_val_label_mat.pt\",\n",
    "    \"SAbDab test labels (test vs test)\": \"ablangpdb_test_label_mat.pt\",\n",
    "    \"DMS validation labels (test vs test)\": \"dms_val_label_mat.pt\",\n",
    "    \"DMS test labels (test vs test)\": \"dms_test_label_mat.pt\"\n",
    "}\n",
    "\n",
    "missing_files = []\n",
    "for desc, filepath in all_required_files.items():\n",
    "    if not check_file_exists(filepath, desc):\n",
    "        missing_files.append(filepath)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {len(missing_files)} files are missing:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"  - {file}\")\n",
    "    print(\"\\nProceeding with available files only.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required files are available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 24 model/dataset/metric combinations:\n",
      "  ‚Ä¢ ablangpdb_sabdab_cosine: AbLangPDB on sabdab using cosine\n",
      "  ‚Ä¢ ablangrbd_sabdab_cosine: AbLangRBD on sabdab using cosine\n",
      "  ‚Ä¢ ablangpre_sabdab_cosine: AbLangPre on sabdab using cosine\n",
      "  ‚Ä¢ ablang2_sabdab_cosine: AbLang2 on sabdab using cosine\n",
      "  ‚Ä¢ ablang_heavy_sabdab_cosine: AbLang-Heavy on sabdab using cosine\n",
      "  ‚Ä¢ antiberty_sabdab_cosine: AntiBERTy on sabdab using cosine\n",
      "  ‚Ä¢ balm_sabdab_cosine: BALM on sabdab using cosine\n",
      "  ‚Ä¢ esm2_sabdab_cosine: ESM-2 on sabdab using cosine\n",
      "  ‚Ä¢ igbert_sabdab_cosine: IgBERT on sabdab using cosine\n",
      "  ‚Ä¢ parapred_sabdab_cosine: Parapred on sabdab using cosine\n",
      "  ‚Ä¢ seqid_sabdab: SEQID on sabdab using seq_identity\n",
      "  ‚Ä¢ cdrh3id_sabdab: CDRH3ID on sabdab using cdrh3_identity\n",
      "  ‚Ä¢ ablangpdb_dms_cosine: AbLangPDB on dms using cosine\n",
      "  ‚Ä¢ ablangrbd_dms_cosine: AbLangRBD on dms using cosine\n",
      "  ‚Ä¢ ablangpre_dms_cosine: AbLangPre on dms using cosine\n",
      "  ‚Ä¢ ablang2_dms_cosine: AbLang2 on dms using cosine\n",
      "  ‚Ä¢ ablang_heavy_dms_cosine: AbLang-Heavy on dms using cosine\n",
      "  ‚Ä¢ antiberty_dms_cosine: AntiBERTy on dms using cosine\n",
      "  ‚Ä¢ balm_dms_cosine: BALM on dms using cosine\n",
      "  ‚Ä¢ esm2_dms_cosine: ESM-2 on dms using cosine\n",
      "  ‚Ä¢ igbert_dms_cosine: IgBERT on dms using cosine\n",
      "  ‚Ä¢ parapred_dms_cosine: Parapred on dms using cosine\n",
      "  ‚Ä¢ seqid_dms: SEQID on dms using seq_identity\n",
      "  ‚Ä¢ cdrh3id_dms: CDRH3ID on dms using cdrh3_identity\n",
      "\n",
      "üìù Note: ABodyBuilder2 DTW configurations have been excluded from this analysis\n"
     ]
    }
   ],
   "source": [
    "# Complete configuration for all model/dataset/metric combinations\n",
    "CONFIGS = {\n",
    "    # SAbDab Dataset Configurations\n",
    "    \"ablangpdb_sabdab_cosine\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPDB\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablangrbd_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablangrbd.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangRBD\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablangpre_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablangpre.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPre\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablang2_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablang2.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLang2\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"ablang_heavy_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_ablang-heavy.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLang-Heavy\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"antiberty_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_antiberty.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"AntiBERTy\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"balm_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_balm.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"BALM\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"esm2_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_esm-2.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"ESM-2\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"igbert_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_igbert.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"IgBERT\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"parapred_sabdab_cosine\": {\n",
    "        \"df_path\": \"sabdab_embeddedby_parapred.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"Parapred\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"seqid_sabdab\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"SEQID\",\n",
    "        \"score_type\": \"seq_identity\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \"cdrh3id_sabdab\": {\n",
    "        \"df_path\": \"ablangpdb_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"ablangpdb_val_label_mat.pt\",\n",
    "        \"labels_test\": \"ablangpdb_test_label_mat.pt\",\n",
    "        \"model_name\": \"CDRH3ID\",\n",
    "        \"score_type\": \"cdrh3_identity\",\n",
    "        \"function\": calculate_metrics.get_metrics,\n",
    "        \"dataset_type\": \"sabdab\"\n",
    "    },\n",
    "    \n",
    "    # DMS Dataset Configurations\n",
    "    \"ablangpdb_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablangpdb.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPDB\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablangrbd_dms_cosine\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangRBD\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablangpre_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablangpre.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLangPre\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablang2_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablang2.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLang2\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"ablang_heavy_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_ablang-heavy.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AbLang-Heavy\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"antiberty_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_antiberty.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"AntiBERTy\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"balm_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_balm.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"BALM\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"esm2_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_esm-2.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"ESM-2\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"igbert_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_igbert.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"IgBERT\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"parapred_dms_cosine\": {\n",
    "        \"df_path\": \"dms_embeddedby_parapred.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"Parapred\",\n",
    "        \"score_type\": \"cosine\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"seqid_dms\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"SEQID\",\n",
    "        \"score_type\": \"seq_identity\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    },\n",
    "    \"cdrh3id_dms\": {\n",
    "        \"df_path\": \"ablangrbd_renameddatasets.parquet\",\n",
    "        \"labels_val\": \"dms_val_label_mat.pt\",\n",
    "        \"labels_test\": \"dms_test_label_mat.pt\",\n",
    "        \"model_name\": \"CDRH3ID\",\n",
    "        \"score_type\": \"cdrh3_identity\",\n",
    "        \"function\": calculate_metrics_dms.get_metrics_dms,\n",
    "        \"dataset_type\": \"dms\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured {len(CONFIGS)} model/dataset/metric combinations:\")\n",
    "for name, config in CONFIGS.items():\n",
    "    print(f\"  ‚Ä¢ {name}: {config['model_name']} on {config['dataset_type']} using {config['score_type']}\")\n",
    "    \n",
    "print(f\"\\nüìù Note: ABodyBuilder2 DTW configurations have been excluded from this analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Available Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ablangpdb_sabdab_cosine: Ready to run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ablangrbd_sabdab_cosine: Ready to run\n",
      "‚úÖ ablangpre_sabdab_cosine: Ready to run\n",
      "‚úÖ ablang2_sabdab_cosine: Ready to run\n",
      "‚úÖ ablang_heavy_sabdab_cosine: Ready to run\n",
      "‚úÖ antiberty_sabdab_cosine: Ready to run\n",
      "‚úÖ balm_sabdab_cosine: Ready to run\n",
      "‚úÖ esm2_sabdab_cosine: Ready to run\n",
      "‚úÖ igbert_sabdab_cosine: Ready to run\n",
      "‚úÖ parapred_sabdab_cosine: Ready to run\n",
      "‚úÖ seqid_sabdab: Ready to run\n",
      "‚úÖ cdrh3id_sabdab: Ready to run\n",
      "‚úÖ ablangpdb_dms_cosine: Ready to run\n",
      "‚úÖ ablangrbd_dms_cosine: Ready to run\n",
      "‚úÖ ablangpre_dms_cosine: Ready to run\n",
      "‚úÖ ablang2_dms_cosine: Ready to run\n",
      "‚úÖ ablang_heavy_dms_cosine: Ready to run\n",
      "‚úÖ antiberty_dms_cosine: Ready to run\n",
      "‚úÖ balm_dms_cosine: Ready to run\n",
      "‚úÖ esm2_dms_cosine: Ready to run\n",
      "‚úÖ igbert_dms_cosine: Ready to run\n",
      "‚úÖ parapred_dms_cosine: Ready to run\n",
      "‚úÖ seqid_dms: Ready to run\n",
      "‚úÖ cdrh3id_dms: Ready to run\n",
      "\n",
      "üìä Summary:\n",
      "  ‚Ä¢ Available configurations: 24/24\n",
      "  ‚Ä¢ Missing configurations: 0\n"
     ]
    }
   ],
   "source": [
    "# Check which configurations can actually run based on available files\n",
    "available_configs = {}\n",
    "missing_configs = []\n",
    "\n",
    "for config_name, config in CONFIGS.items():\n",
    "    files_to_check = [config[\"df_path\"], config[\"labels_val\"], config[\"labels_test\"]]\n",
    "    missing_files = [f for f in files_to_check if not os.path.exists(f)]\n",
    "    \n",
    "    if not missing_files:\n",
    "        available_configs[config_name] = config\n",
    "        print(f\"‚úÖ {config_name}: Ready to run\")\n",
    "    else:\n",
    "        missing_configs.append(config_name)\n",
    "        print(f\"‚ùå {config_name}: Missing files - {missing_files}\")\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  ‚Ä¢ Available configurations: {len(available_configs)}/{len(CONFIGS)}\")\n",
    "print(f\"  ‚Ä¢ Missing configurations: {len(missing_configs)}\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"\\n‚ö†Ô∏è Configurations that will be skipped: {', '.join(missing_configs)}\")\n",
    "\n",
    "if not available_configs:\n",
    "    raise RuntimeError(\"‚ùå No configurations are available to run!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Pre-computed Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Computing fresh thresholds for all configurations\n",
      "üìù Note: Test vs test comparisons require new threshold optimization\n"
     ]
    }
   ],
   "source": [
    "# Optional: Pre-computed thresholds to skip threshold optimization\n",
    "# NOTE: Thresholds from train vs test analysis are not valid for test vs test comparisons\n",
    "# These thresholds need to be recalculated for the new comparison type\n",
    "\n",
    "PRECOMPUTED_THRESHOLDS = {\n",
    "    # Thresholds will need to be recalculated for test vs test comparisons\n",
    "    # The original thresholds were optimized for train vs test, not test vs test\n",
    "    # Leave empty to force recalculation\n",
    "}\n",
    "\n",
    "use_precomputed = len(PRECOMPUTED_THRESHOLDS) > 0\n",
    "if use_precomputed:\n",
    "    print(f\"üîÑ Using precomputed thresholds for {len(PRECOMPUTED_THRESHOLDS)} configurations\")\n",
    "    for config_name, thresholds in PRECOMPUTED_THRESHOLDS.items():\n",
    "        print(f\"  ‚Ä¢ {config_name}: {thresholds}\")\n",
    "else:\n",
    "    print(\"üÜï Computing fresh thresholds for all configurations\")\n",
    "    print(\"üìù Note: Test vs test comparisons require new threshold optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "DATASET",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ef0161e9-02df-4b90-ada4-c6cf6ee645df",
       "rows": [
        [
         "TRAIN",
         "1517"
        ],
        [
         "TEST",
         "202"
        ],
        [
         "VAL",
         "190"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "DATASET\n",
       "TRAIN    1517\n",
       "TEST      202\n",
       "VAL       190\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('ablangpdb_renameddatasets.parquet')\n",
    "df[\"DATASET\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comprehensive Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE BENCHMARK EXECUTION\n",
      "======================================================================\n",
      "Total configurations to run: 24\n",
      "Recalculate summary metrics: False\n",
      "\n",
      "======================================================================\n",
      "[1/24] Running: ablangpdb_sabdab_cosine\n",
      "Model: AbLangPDB, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.4388\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.2364\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6768, Average Precision: 0.3275, F1 Score: 0.3503\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6800, Average Precision: 0.3360, F1 Score: 0.3259\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangPDB_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLangPDB_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [1/24] ablangpdb_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[2/24] Running: ablangrbd_sabdab_cosine\n",
      "Model: AbLangRBD, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.7961\n",
      "Optimal F1 Threshold for Antigen (>=0.2): -0.1440\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6138, Average Precision: 0.2466, F1 Score: 0.2226\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5301, Average Precision: 0.2063, F1 Score: 0.1874\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangRBD_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLangRBD_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [2/24] ablangrbd_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[3/24] Running: ablangpre_sabdab_cosine\n",
      "Model: AbLangPre, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.8113\n",
      "Optimal F1 Threshold for Antigen (>=0.2): -0.0926\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6804, Average Precision: 0.2111, F1 Score: 0.1876\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6315, Average Precision: 0.2472, F1 Score: 0.1979\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangPre_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLangPre_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [3/24] ablangpre_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[4/24] Running: ablang2_sabdab_cosine\n",
      "Model: AbLang2, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9362\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.1976\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.5552, Average Precision: 0.0828, F1 Score: 0.1153\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5339, Average Precision: 0.1452, F1 Score: 0.1980\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLang2_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLang2_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [4/24] ablang2_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[5/24] Running: ablang_heavy_sabdab_cosine\n",
      "Model: AbLang-Heavy, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.8132\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.6648\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6799, Average Precision: 0.1949, F1 Score: 0.2341\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6423, Average Precision: 0.2329, F1 Score: 0.2615\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLang-Heavy_sabdab_ep_summarymetrics.txt and output_csvs_test/AbLang-Heavy_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [5/24] ablang_heavy_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[6/24] Running: antiberty_sabdab_cosine\n",
      "Model: AntiBERTy, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.8221\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.6469\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.7052, Average Precision: 0.2640, F1 Score: 0.2991\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6596, Average Precision: 0.2654, F1 Score: 0.2509\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AntiBERTy_sabdab_ep_summarymetrics.txt and output_csvs_test/AntiBERTy_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [6/24] antiberty_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[7/24] Running: balm_sabdab_cosine\n",
      "Model: BALM, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9446\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.9226\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6334, Average Precision: 0.1882, F1 Score: 0.1879\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5979, Average Precision: 0.2157, F1 Score: 0.2406\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/BALM_sabdab_ep_summarymetrics.txt and output_csvs_test/BALM_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [7/24] balm_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[8/24] Running: esm2_sabdab_cosine\n",
      "Model: ESM-2, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9953\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.9902\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6047, Average Precision: 0.1721, F1 Score: 0.2039\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5611, Average Precision: 0.1870, F1 Score: 0.1965\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/ESM-2_sabdab_ep_summarymetrics.txt and output_csvs_test/ESM-2_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [8/24] esm2_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[9/24] Running: igbert_sabdab_cosine\n",
      "Model: IgBERT, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9752\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.9655\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6272, Average Precision: 0.1382, F1 Score: 0.1837\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.6011, Average Precision: 0.2007, F1 Score: 0.2316\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/IgBERT_sabdab_ep_summarymetrics.txt and output_csvs_test/IgBERT_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [9/24] igbert_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[10/24] Running: parapred_sabdab_cosine\n",
      "Model: Parapred, Dataset: sabdab, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.9985\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.9981\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6320, Average Precision: 0.1813, F1 Score: 0.1465\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5889, Average Precision: 0.1958, F1 Score: 0.2128\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/Parapred_sabdab_ep_summarymetrics.txt and output_csvs_test/Parapred_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [10/24] parapred_sabdab_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[11/24] Running: seqid_sabdab\n",
      "Model: SEQID, Dataset: sabdab, Score: seq_identity\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'seq_identity' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.5576\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.3054\n",
      "Preparing data with score_type: 'seq_identity' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6876, Average Precision: 0.2628, F1 Score: 0.1259\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5832, Average Precision: 0.2291, F1 Score: 0.1979\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/SEQID_sabdab_ep_summarymetrics.txt and output_csvs_test/SEQID_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [11/24] seqid_sabdab completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[12/24] Running: cdrh3id_sabdab\n",
      "Model: CDRH3ID, Dataset: sabdab, Score: cdrh3_identity\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 thresholds using VAL data ---\n",
      "Preparing data with score_type: 'cdrh3_identity' for VAL vs VAL...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([190, 190]), Using 17703 pairs (lower triangle)\n",
      "Optimal F1 Threshold for Epitope (>=0.5): 0.3810\n",
      "Optimal F1 Threshold for Antigen (>=0.2): 0.0000\n",
      "Preparing data with score_type: 'cdrh3_identity' for TEST vs TEST...\n",
      "  üîÑ Square matrix mode: Using lower triangle (excluding diagonal)\n",
      "  üìä Matrix shape: torch.Size([202, 202]), Using 19910 pairs (lower triangle)\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Positive label >= 0.5) ---\n",
      "ROC-AUC: 0.6390, Average Precision: 0.2341, F1 Score: 0.1741\n",
      "\n",
      "--- Analyzing Antigen-level performance on TEST data (Positive label >= 0.2) ---\n",
      "ROC-AUC: 0.5415, Average Precision: 0.1978, F1 Score: 0.1979\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/CDRH3ID_sabdab_ep_summarymetrics.txt and output_csvs_test/CDRH3ID_sabdab_ag_summarymetrics.txt\n",
      "\n",
      "‚úÖ [12/24] cdrh3id_sabdab completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[13/24] Running: ablangpdb_dms_cosine\n",
      "Model: AbLangPDB, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: -0.0278\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5348, Average Precision: 0.1375, F1 Score: 0.1979\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangPDB_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [13/24] ablangpdb_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[14/24] Running: ablangrbd_dms_cosine\n",
      "Model: AbLangRBD, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.7934\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.7279, Average Precision: 0.3944, F1 Score: 0.3859\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangRBD_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [14/24] ablangrbd_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[15/24] Running: ablangpre_dms_cosine\n",
      "Model: AbLangPre, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6833\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5698, Average Precision: 0.1626, F1 Score: 0.1965\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLangPre_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [15/24] ablangpre_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[16/24] Running: ablang2_dms_cosine\n",
      "Model: AbLang2, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.5164\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5277, Average Precision: 0.1206, F1 Score: 0.1978\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLang2_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [16/24] ablang2_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[17/24] Running: ablang_heavy_dms_cosine\n",
      "Model: AbLang-Heavy, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6799\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5603, Average Precision: 0.1545, F1 Score: 0.1980\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AbLang-Heavy_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [17/24] ablang_heavy_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[18/24] Running: antiberty_dms_cosine\n",
      "Model: AntiBERTy, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6837\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5675, Average Precision: 0.1610, F1 Score: 0.2060\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/AntiBERTy_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [18/24] antiberty_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[19/24] Running: balm_dms_cosine\n",
      "Model: BALM, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.9387\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5545, Average Precision: 0.1498, F1 Score: 0.2002\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/BALM_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [19/24] balm_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[20/24] Running: esm2_dms_cosine\n",
      "Model: ESM-2, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.9933\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5576, Average Precision: 0.1650, F1 Score: 0.1966\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/ESM-2_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [20/24] esm2_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[21/24] Running: igbert_dms_cosine\n",
      "Model: IgBERT, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6031\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5373, Average Precision: 0.1328, F1 Score: 0.1969\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/IgBERT_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [21/24] igbert_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[22/24] Running: parapred_dms_cosine\n",
      "Model: Parapred, Dataset: dms, Score: cosine\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cosine' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.9909\n",
      "Preparing data with score_type: 'cosine' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5516, Average Precision: 0.1438, F1 Score: 0.1969\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/Parapred_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [22/24] parapred_dms_cosine completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[23/24] Running: seqid_dms\n",
      "Model: SEQID, Dataset: dms, Score: seq_identity\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'seq_identity' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.6708\n",
      "Preparing data with score_type: 'seq_identity' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5501, Average Precision: 0.1765, F1 Score: 0.1918\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/SEQID_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [23/24] seqid_dms completed successfully!\n",
      "\n",
      "======================================================================\n",
      "[24/24] Running: cdrh3id_dms\n",
      "Model: CDRH3ID, Dataset: dms, Score: cdrh3_identity\n",
      "======================================================================\n",
      "üìã Some summary files missing, proceeding with calculation...\n",
      "üîÑ Running with parameters:\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Final evaluation: TEST vs TEST\n",
      "  ‚Ä¢ Square matrix mode: ENABLED\n",
      "\n",
      "--- Finding optimal F1 threshold using VAL data ---\n",
      "Preparing data with score_type: 'cdrh3_identity' for VAL vs VAL...\n",
      "Optimal F1 Threshold for Epitope matching: 0.1905\n",
      "Preparing data with score_type: 'cdrh3_identity' for TEST vs TEST...\n",
      "\n",
      "--- Analyzing Epitope-level performance on TEST data (Binary epitope matching) ---\n",
      "ROC-AUC: 0.5273, Average Precision: 0.1285, F1 Score: 0.1971\n",
      "\n",
      "Results saved to output_csvs_test\n",
      "Summary metrics saved to output_csvs_test/CDRH3ID_dms_summarymetrics.txt\n",
      "\n",
      "‚úÖ [24/24] cdrh3id_dms completed successfully!\n",
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE BENCHMARK EXECUTION SUMMARY\n",
      "======================================================================\n",
      "ablangpdb_sabdab_cosine        ‚úÖ Success\n",
      "ablangrbd_sabdab_cosine        ‚úÖ Success\n",
      "ablangpre_sabdab_cosine        ‚úÖ Success\n",
      "ablang2_sabdab_cosine          ‚úÖ Success\n",
      "ablang_heavy_sabdab_cosine     ‚úÖ Success\n",
      "antiberty_sabdab_cosine        ‚úÖ Success\n",
      "balm_sabdab_cosine             ‚úÖ Success\n",
      "esm2_sabdab_cosine             ‚úÖ Success\n",
      "igbert_sabdab_cosine           ‚úÖ Success\n",
      "parapred_sabdab_cosine         ‚úÖ Success\n",
      "seqid_sabdab                   ‚úÖ Success\n",
      "cdrh3id_sabdab                 ‚úÖ Success\n",
      "ablangpdb_dms_cosine           ‚úÖ Success\n",
      "ablangrbd_dms_cosine           ‚úÖ Success\n",
      "ablangpre_dms_cosine           ‚úÖ Success\n",
      "ablang2_dms_cosine             ‚úÖ Success\n",
      "ablang_heavy_dms_cosine        ‚úÖ Success\n",
      "antiberty_dms_cosine           ‚úÖ Success\n",
      "balm_dms_cosine                ‚úÖ Success\n",
      "esm2_dms_cosine                ‚úÖ Success\n",
      "igbert_dms_cosine              ‚úÖ Success\n",
      "parapred_dms_cosine            ‚úÖ Success\n",
      "seqid_dms                      ‚úÖ Success\n",
      "cdrh3id_dms                    ‚úÖ Success\n",
      "\n",
      "üìä Results:\n",
      "  ‚Ä¢ Successful: 24/24\n",
      "  ‚Ä¢ Failed: 0\n",
      "\n",
      "üéâ All available configurations completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Execute all available configurations\n",
    "execution_results = {}\n",
    "failed_configs = []\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"COMPREHENSIVE BENCHMARK EXECUTION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total configurations to run: {len(available_configs)}\")\n",
    "print(f\"Recalculate summary metrics: {RECALCULATE_SUMMARYMETRICS}\")\n",
    "\n",
    "for i, (config_name, config) in enumerate(available_configs.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i}/{len(available_configs)}] Running: {config_name}\")\n",
    "    print(f\"Model: {config['model_name']}, Dataset: {config['dataset_type']}, Score: {config['score_type']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check if summary files already exist and flag is False\n",
    "    if not RECALCULATE_SUMMARYMETRICS:\n",
    "        summary_files = get_summary_file_paths(config['model_name'], config['dataset_type'], OUTPUT_FOLDER)\n",
    "        all_summaries_exist = all(os.path.exists(f) for f in summary_files)\n",
    "        \n",
    "        if all_summaries_exist:\n",
    "            print(f\"üìã Summary files already exist, skipping recalculation...\")\n",
    "            read_and_display_summary_results(summary_files, config['model_name'], config['dataset_type'])\n",
    "            execution_results[config_name] = \"‚úÖ Loaded from existing files\"\n",
    "            print(f\"\\n‚úÖ [{i}/{len(available_configs)}] {config_name} loaded from existing files!\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"üìã Some summary files missing, proceeding with calculation...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare arguments for TEST vs TEST comparisons\n",
    "        args = {\n",
    "            \"df_path\": config[\"df_path\"],\n",
    "            \"labels_file_val\": config[\"labels_val\"],\n",
    "            \"labels_file_test\": config[\"labels_test\"],\n",
    "            \"score_type\": config[\"score_type\"],\n",
    "            \"model_name\": config[\"model_name\"],\n",
    "            \"output_folder\": OUTPUT_FOLDER,\n",
    "            # CRITICAL FIX: Use correct dataset parameters for test vs test\n",
    "            \"dataset1\": \"VAL\",           # Use VAL for threshold optimization (VAL vs VAL)\n",
    "            \"dataset2_val\": \"VAL\",       # VAL vs VAL for F1 threshold finding\n",
    "            \"dataset2_test\": \"TEST\",     # TEST vs TEST for final evaluation\n",
    "            \"use_square_matrices\": True  # Enable proper square matrix handling\n",
    "        }\n",
    "        \n",
    "        # Add matrix file paths for ABodyBuilder2 DTW configurations (if they exist)\n",
    "        if config[\"score_type\"] == \"abodybuilder2_dtw_cdrs\":\n",
    "            args[\"matrix_file_val\"] = config[\"matrix_file_val\"]\n",
    "            args[\"matrix_file_test\"] = config[\"matrix_file_test\"]\n",
    "        \n",
    "        # Add precomputed thresholds if available\n",
    "        if config_name in PRECOMPUTED_THRESHOLDS:\n",
    "            thresholds = PRECOMPUTED_THRESHOLDS[config_name]\n",
    "            if config[\"dataset_type\"] == \"sabdab\":\n",
    "                if \"epitope_threshold\" in thresholds:\n",
    "                    args[\"epitope_threshold\"] = thresholds[\"epitope_threshold\"]\n",
    "                if \"antigen_threshold\" in thresholds:\n",
    "                    args[\"antigen_threshold\"] = thresholds[\"antigen_threshold\"]\n",
    "            elif config[\"dataset_type\"] == \"dms\":\n",
    "                if \"epitope_threshold\" in thresholds:\n",
    "                    args[\"epitope_threshold\"] = thresholds[\"epitope_threshold\"]\n",
    "        \n",
    "        # Execute the benchmark\n",
    "        print(f\"üîÑ Running with parameters:\")\n",
    "        print(f\"  ‚Ä¢ Threshold optimization: VAL vs VAL\")\n",
    "        print(f\"  ‚Ä¢ Final evaluation: TEST vs TEST\")\n",
    "        print(f\"  ‚Ä¢ Square matrix mode: ENABLED\")\n",
    "        \n",
    "        config[\"function\"](**args)\n",
    "        \n",
    "        execution_results[config_name] = \"‚úÖ Success\"\n",
    "        print(f\"\\n‚úÖ [{i}/{len(available_configs)}] {config_name} completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error: {str(e)}\"\n",
    "        execution_results[config_name] = error_msg\n",
    "        failed_configs.append(config_name)\n",
    "        print(f\"\\n‚ùå [{i}/{len(available_configs)}] {config_name} failed: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPREHENSIVE BENCHMARK EXECUTION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for config_name, result in execution_results.items():\n",
    "    print(f\"{config_name:30} {result}\")\n",
    "\n",
    "successful_configs = len(available_configs) - len(failed_configs)\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  ‚Ä¢ Successful: {successful_configs}/{len(available_configs)}\")\n",
    "print(f\"  ‚Ä¢ Failed: {len(failed_configs)}\")\n",
    "\n",
    "if failed_configs:\n",
    "    print(f\"\\n‚ö†Ô∏è Failed configurations: {', '.join(failed_configs)}\")\n",
    "else:\n",
    "    print(\"\\nüéâ All available configurations completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive Excel Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Generating summary statistics...\n",
      "\n",
      "=== Summary Statistics ===\n",
      "Total summary files found: 36\n",
      "Unique models: 12 (AbLang-Heavy, AbLang2, AbLangPDB, AbLangPre, AbLangRBD, AntiBERTy, BALM, CDRH3ID, ESM-2, IgBERT, Parapred, SEQID)\n",
      "Unique datasets: 3 (dms, sabdab_ag, sabdab_ep)\n",
      "Unique score types: 3 (cdrh3_identity, cosine, seq_identity)\n",
      "Best ROC_AUC: AbLangRBD on dms (0.7279)\n",
      "Best Average_Precision: AbLangRBD on dms (0.3944)\n",
      "Best F1_Score: AbLangRBD on dms (0.3859)\n",
      "\n",
      "======================================================================\n",
      "GENERATING COMPREHENSIVE EXCEL REPORT\n",
      "======================================================================\n",
      "Collecting summary metrics from output_csvs_test...\n",
      "Found 36 summary files\n",
      "Creating pivot table...\n",
      "Pivot table created with 12 models and 9 metric columns\n",
      "Ranking values for formatting...\n",
      "Exporting to Excel: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "‚úÖ Excel file generated successfully: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "\n",
      "üéâ Comprehensive Excel report generated successfully!\n",
      "üìÅ File location: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "üìè File size: 5,887 bytes\n",
      "\n",
      "üìñ Excel Report Contents:\n",
      "  ‚Ä¢ Models as rows (AbLangPDB, AbLangRBD, AbLangPre, SEQID, CDRH3ID)\n",
      "  ‚Ä¢ Datasets grouped as column headers (SAbDab, DMS)\n",
      "  ‚Ä¢ Metrics: ROC-AUC, Average Precision, F1 Score\n",
      "  ‚Ä¢ Best performance: Bold formatting\n",
      "  ‚Ä¢ Second best: Italic formatting\n",
      "  ‚Ä¢ Values rounded to 4 decimal places\n"
     ]
    }
   ],
   "source": [
    "# Generate summary statistics\n",
    "print(\"\\nüìä Generating summary statistics...\")\n",
    "print_summary_stats(OUTPUT_FOLDER)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING COMPREHENSIVE EXCEL REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Generate the Excel file\n",
    "    excel_path = generate_results_excel(\n",
    "        output_folder=OUTPUT_FOLDER,\n",
    "        excel_filename=EXCEL_FILENAME\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ Comprehensive Excel report generated successfully!\")\n",
    "    print(f\"üìÅ File location: {excel_path}\")\n",
    "    print(f\"üìè File size: {os.path.getsize(excel_path):,} bytes\")\n",
    "    \n",
    "    # Provide usage instructions\n",
    "    print(f\"\\nüìñ Excel Report Contents:\")\n",
    "    print(f\"  ‚Ä¢ Models as rows (AbLangPDB, AbLangRBD, AbLangPre, SEQID, CDRH3ID)\")\n",
    "    print(f\"  ‚Ä¢ Datasets grouped as column headers (SAbDab, DMS)\")\n",
    "    print(f\"  ‚Ä¢ Metrics: ROC-AUC, Average Precision, F1 Score\")\n",
    "    print(f\"  ‚Ä¢ Best performance: Bold formatting\")\n",
    "    print(f\"  ‚Ä¢ Second best: Italic formatting\")\n",
    "    print(f\"  ‚Ä¢ Values rounded to 4 decimal places\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating Excel report: {str(e)}\")\n",
    "    print(\"\\nDebugging information:\")\n",
    "    print(f\"  ‚Ä¢ Output folder: {OUTPUT_FOLDER}\")\n",
    "    print(f\"  ‚Ä¢ Files in folder: {len(os.listdir(OUTPUT_FOLDER))}\")\n",
    "    \n",
    "    # List summary files found\n",
    "    import glob\n",
    "    summary_files = glob.glob(os.path.join(OUTPUT_FOLDER, \"*summarymetrics.txt\"))\n",
    "    print(f\"  ‚Ä¢ Summary files found: {len(summary_files)}\")\n",
    "    for f in summary_files[:5]:  # Show first 5\n",
    "        print(f\"    - {os.path.basename(f)}\")\n",
    "    if len(summary_files) > 5:\n",
    "        print(f\"    - ... and {len(summary_files)-5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST vs TEST PIPELINE COMPLETION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üîß Configuration:\n",
      "  ‚Ä¢ Analysis type: TEST vs TEST comparisons\n",
      "  ‚Ä¢ Threshold optimization: VAL vs VAL\n",
      "  ‚Ä¢ Recalculated embeddings: False\n",
      "  ‚Ä¢ Used precomputed thresholds: False\n",
      "  ‚Ä¢ Batch size: 256\n",
      "\n",
      "üìà Benchmarking Results:\n",
      "  ‚Ä¢ Total configurations possible: 24\n",
      "  ‚Ä¢ Configurations attempted: 24\n",
      "  ‚Ä¢ Successful runs: 24\n",
      "  ‚Ä¢ Failed runs: 0\n",
      "\n",
      "üìä Excel Report:\n",
      "  ‚Ä¢ Status: ‚úÖ Generated successfully\n",
      "  ‚Ä¢ Location: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "  ‚Ä¢ Ready for analysis and sharing\n",
      "\n",
      "üî¨ Models Configured for Benchmarking:\n",
      "  ‚Ä¢ AbLang-Heavy (cosine)\n",
      "  ‚Ä¢ AbLang2 (cosine)\n",
      "  ‚Ä¢ AbLangPDB (cosine)\n",
      "  ‚Ä¢ AbLangPre (cosine)\n",
      "  ‚Ä¢ AbLangRBD (cosine)\n",
      "  ‚Ä¢ AntiBERTy (cosine)\n",
      "  ‚Ä¢ BALM (cosine)\n",
      "  ‚Ä¢ CDRH3ID (cdrh3_identity)\n",
      "  ‚Ä¢ ESM-2 (cosine)\n",
      "  ‚Ä¢ IgBERT (cosine)\n",
      "  ‚Ä¢ Parapred (cosine)\n",
      "  ‚Ä¢ SEQID (seq_identity)\n",
      "\n",
      "üìä Datasets Configured:\n",
      "  ‚Ä¢ DMS\n",
      "  ‚Ä¢ SABDAB\n",
      "\n",
      "üéØ Key Differences from Train vs Test Analysis:\n",
      "  1. üîÑ Compares TEST antibodies against other TEST antibodies\n",
      "  2. üéØ Uses VAL vs VAL for F1 threshold optimization\n",
      "  3. üìä Uses test_label_mat.pt and val_label_mat.pt matrices\n",
      "  4. üö´ Excludes ABodyBuilder2 DTW structural similarity calculations\n",
      "\n",
      "üéØ Next Steps:\n",
      "  1. üìä Open the Excel report for comprehensive performance comparison\n",
      "  2. üîç Compare with train vs test results to understand differences\n",
      "  3. üìà Analyze performance patterns in test vs test setting\n",
      "  4. üìã Share results with your research team\n",
      "  5. üìù Consider implications for real-world performance\n",
      "\n",
      "üí° Model Coverage Summary:\n",
      "  ‚Ä¢ Total unique models: 12\n",
      "  ‚Ä¢ Embedding-based models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2,\n",
      "    AbLang-Heavy, AntiBERTy, BALM, ESM-2, IgBERT, Parapred\n",
      "  ‚Ä¢ Sequence-based models: SEQID, CDRH3ID\n",
      "  ‚Ä¢ ABodyBuilder2 DTW: Excluded from this analysis\n",
      "  ‚Ä¢ Total configurations: 24\n",
      "\n",
      "üèÅ Test vs test benchmarking pipeline completed!\n",
      "\n",
      "üìÑ Report: output_csvs_test/comprehensive_benchmarking_results_test.xlsx\n",
      "üìÅ Output folder: output_csvs_test\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST vs TEST PIPELINE COMPLETION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüîß Configuration:\")\n",
    "print(f\"  ‚Ä¢ Analysis type: TEST vs TEST comparisons\")\n",
    "print(f\"  ‚Ä¢ Threshold optimization: VAL vs VAL\")\n",
    "print(f\"  ‚Ä¢ Recalculated embeddings: {RECALCULATE_EMBEDDINGS}\")\n",
    "print(f\"  ‚Ä¢ Used precomputed thresholds: {use_precomputed}\")\n",
    "print(f\"  ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "print(f\"\\nüìà Benchmarking Results:\")\n",
    "print(f\"  ‚Ä¢ Total configurations possible: {len(CONFIGS)}\")\n",
    "print(f\"  ‚Ä¢ Configurations attempted: {len(available_configs)}\")\n",
    "print(f\"  ‚Ä¢ Successful runs: {successful_configs}\")\n",
    "print(f\"  ‚Ä¢ Failed runs: {len(failed_configs)}\")\n",
    "\n",
    "if os.path.exists(os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)):\n",
    "    print(f\"\\nüìä Excel Report:\")\n",
    "    print(f\"  ‚Ä¢ Status: ‚úÖ Generated successfully\")\n",
    "    print(f\"  ‚Ä¢ Location: {os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)}\")\n",
    "    print(f\"  ‚Ä¢ Ready for analysis and sharing\")\n",
    "else:\n",
    "    print(f\"\\nüìä Excel Report:\")\n",
    "    print(f\"  ‚Ä¢ Status: ‚ùå Generation failed\")\n",
    "    print(f\"  ‚Ä¢ Check error messages above\")\n",
    "\n",
    "print(f\"\\nüî¨ Models Configured for Benchmarking:\")\n",
    "all_models = set()\n",
    "for config_name, config in CONFIGS.items():\n",
    "    all_models.add(f\"{config['model_name']} ({config['score_type']})\")\n",
    "        \n",
    "for model in sorted(all_models):\n",
    "    print(f\"  ‚Ä¢ {model}\")\n",
    "\n",
    "print(f\"\\nüìä Datasets Configured:\")\n",
    "datasets_configured = set()\n",
    "for config_name, config in CONFIGS.items():\n",
    "    datasets_configured.add(config['dataset_type'].upper())\n",
    "        \n",
    "for dataset in sorted(datasets_configured):\n",
    "    print(f\"  ‚Ä¢ {dataset}\")\n",
    "\n",
    "print(f\"\\nüéØ Key Differences from Train vs Test Analysis:\")\n",
    "print(f\"  1. üîÑ Compares TEST antibodies against other TEST antibodies\")\n",
    "print(f\"  2. üéØ Uses VAL vs VAL for F1 threshold optimization\")\n",
    "print(f\"  3. üìä Uses test_label_mat.pt and val_label_mat.pt matrices\")\n",
    "print(f\"  4. üö´ Excludes ABodyBuilder2 DTW structural similarity calculations\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. üìä Open the Excel report for comprehensive performance comparison\")\n",
    "print(f\"  2. üîç Compare with train vs test results to understand differences\")\n",
    "print(f\"  3. üìà Analyze performance patterns in test vs test setting\")\n",
    "print(f\"  4. üìã Share results with your research team\")\n",
    "print(f\"  5. üìù Consider implications for real-world performance\")\n",
    "\n",
    "if failed_configs:\n",
    "    print(f\"\\n‚ö†Ô∏è Failed Configurations to Investigate:\")\n",
    "    for config in failed_configs:\n",
    "        print(f\"  ‚Ä¢ {config}: {execution_results[config]}\")\n",
    "\n",
    "if missing_configs:\n",
    "    print(f\"\\n‚ùì Configurations Not Attempted (Missing Files):\")\n",
    "    for config in missing_configs:\n",
    "        print(f\"  ‚Ä¢ {config}\")\n",
    "\n",
    "print(f\"\\nüí° Model Coverage Summary:\")\n",
    "print(f\"  ‚Ä¢ Total unique models: {len(all_models)}\")\n",
    "print(f\"  ‚Ä¢ Embedding-based models: AbLangPDB, AbLangRBD, AbLangPre, AbLang2,\")\n",
    "print(f\"    AbLang-Heavy, AntiBERTy, BALM, ESM-2, IgBERT, Parapred\")\n",
    "print(f\"  ‚Ä¢ Sequence-based models: SEQID, CDRH3ID\")\n",
    "print(f\"  ‚Ä¢ ABodyBuilder2 DTW: Excluded from this analysis\")\n",
    "print(f\"  ‚Ä¢ Total configurations: {len(CONFIGS)}\")\n",
    "\n",
    "print(f\"\\nüèÅ Test vs test benchmarking pipeline completed!\")\n",
    "print(f\"\\nüìÑ Report: {os.path.join(OUTPUT_FOLDER, EXCEL_FILENAME)}\")\n",
    "print(f\"üìÅ Output folder: {OUTPUT_FOLDER}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pub_clone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
