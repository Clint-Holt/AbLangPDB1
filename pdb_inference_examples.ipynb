{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AbLangPDB1 Inference Examples\n",
    "\n",
    "This notebook demonstrates how to use AbLangPDB1 to generate embeddings for antibody sequences. AbLangPDB1 creates 1536-dimensional embeddings where antibodies targeting similar epitopes cluster together.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have:\n",
    "1. Downloaded the model weights: `ablangpdb_model.safetensors`\n",
    "2. Installed required packages: `torch`, `pandas`, `transformers`, `safetensors`\n",
    "\n",
    "```bash\n",
    "# Download model weights\n",
    "curl -L \"https://huggingface.co/clint-holt/AbLangPDB1/resolve/main/ablangpdb_model.safetensors?download=true\" -o ablangpdb_model.safetensors\n",
    "\n",
    "# Install dependencies\n",
    "pip install torch pandas transformers safetensors\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "# Data processing imports\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hugging Face Transformers imports\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Local import\n",
    "from ablangpaired_model import AbLangPairedConfig, AbLangPaired\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "These functions help with tokenization and batch processing of antibody sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df: pd.DataFrame, model_config: AbLangPairedConfig) -> TensorDataset:\n",
    "    \"\"\"\n",
    "    Prepare antibody sequences for input to the AbLangPDB1 model.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing antibody sequences with HC_AA and LC_AA columns\n",
    "        model_config: AbLangPairedConfig that tells where to load the tokenizers from\n",
    "        \n",
    "    Returns:\n",
    "        TensorDataset with encoded sequences ready for model input\n",
    "    \"\"\"   \n",
    "    # Filter out sequences that are too long or contain stop codons\n",
    "    # AbLang tokenizers work best with sequences under 157 amino acids\n",
    "    df = df[(df[\"HC_AA\"].apply(lambda aa: (len(aa) < 157) & (\"*\" not in aa))) & \n",
    "            (df[\"LC_AA\"].apply(lambda aa: (len(aa) < 157) & (\"*\" not in aa)))]\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"No valid sequences found after filtering\")\n",
    "    \n",
    "    # Load tokenizers for heavy and light chains\n",
    "    print(\"Loading tokenizers...\")\n",
    "    heavy_tokenizer = AutoTokenizer.from_pretrained(model_config.heavy_model_id, revision=model_config.heavy_revision)\n",
    "    light_tokenizer = AutoTokenizer.from_pretrained(model_config.light_model_id, revision=model_config.light_revision)\n",
    "\n",
    "    # Format sequences for tokenization (add spaces between amino acids)\n",
    "    df.loc[:, \"PREPARED_HC_SEQ\"] = df[\"HC_AA\"].apply(lambda x: \" \".join(list(x)))\n",
    "    df.loc[:, \"PREPARED_LC_SEQ\"] = df[\"LC_AA\"].apply(lambda x: \" \".join(list(x)))\n",
    "\n",
    "    # Tokenize heavy chain sequences\n",
    "    print(\"Tokenizing heavy chain sequences...\")\n",
    "    h_train_tokens = heavy_tokenizer.batch_encode_plus(\n",
    "        df[\"PREPARED_HC_SEQ\"].tolist(), \n",
    "        add_special_tokens=True, \n",
    "        padding='longest', \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize light chain sequences\n",
    "    print(\"Tokenizing light chain sequences...\")\n",
    "    l_train_tokens = light_tokenizer.batch_encode_plus(\n",
    "        df[\"PREPARED_LC_SEQ\"].tolist(), \n",
    "        add_special_tokens=True, \n",
    "        padding='longest', \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    \n",
    "    # Handle unknown tokens by converting them to mask tokens\n",
    "    # This prevents errors during inference\n",
    "    for tokens_dict in [h_train_tokens, l_train_tokens]:\n",
    "        matches = torch.where(tokens_dict['input_ids'] == 24)  # UNK token\n",
    "        if len(matches[0]) > 0:\n",
    "            tokens_dict['input_ids'][matches] = 23  # MASK token\n",
    "            tokens_dict['attention_mask'][matches] = False\n",
    "    \n",
    "    # Create TensorDataset for model input\n",
    "    dataset = TensorDataset(\n",
    "        h_train_tokens['input_ids'].to(torch.int16), \n",
    "        l_train_tokens['input_ids'].to(torch.int16),\n",
    "        h_train_tokens['attention_mask'].to(torch.bool),\n",
    "        l_train_tokens['attention_mask'].to(torch.bool)\n",
    "    )\n",
    "    \n",
    "    print(f\"Created dataset with {len(dataset)} sequences\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def embed_dataloader(dataloader, model, device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate embeddings for all antibodies in the dataloader.\n",
    "    \n",
    "    Args:\n",
    "        dataloader: DataLoader containing tokenized antibody sequences\n",
    "        model: Trained AbLangPDB1 model\n",
    "        device: Device to run inference on (CPU or GPU)\n",
    "        \n",
    "    Returns:\n",
    "        Tensor containing embeddings for all antibodies (shape: N x 1536)\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Preallocate tensor for all embeddings\n",
    "    num_embeddings = len(dataloader.dataset)\n",
    "    embedding_dim = 1536\n",
    "    all_embeds = torch.zeros((num_embeddings, embedding_dim), dtype=torch.float32)\n",
    "    \n",
    "    # Generate embeddings batch by batch\n",
    "    current_batch_index = 0\n",
    "    print(\"Generating embeddings...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for htoks, ltoks, hmasks, lmasks in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            # Move tensors to device\n",
    "            htoks = htoks.to(device)\n",
    "            hmasks = hmasks.to(device)\n",
    "            ltoks = ltoks.to(device) \n",
    "            lmasks = lmasks.to(device)\n",
    "            \n",
    "            # Forward pass to get embeddings\n",
    "            embeds = model(\n",
    "                h_input_ids=htoks, \n",
    "                h_attention_mask=hmasks, \n",
    "                l_input_ids=ltoks, \n",
    "                l_attention_mask=lmasks\n",
    "            )\n",
    "            \n",
    "            # Store embeddings in preallocated tensor\n",
    "            batch_size = embeds.size(0)\n",
    "            all_embeds[current_batch_index:current_batch_index + batch_size] = embeds.detach().cpu()\n",
    "            current_batch_index += batch_size\n",
    "            \n",
    "            # Clean up GPU memory\n",
    "            del htoks, hmasks, ltoks, lmasks, embeds\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"Generated {all_embeds.shape[0]} embeddings of dimension {all_embeds.shape[1]}\")\n",
    "    return all_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model\n",
    "\n",
    "Load the AbLangPDB1 model and tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model weights exist\n",
    "model_path = \"ablangpdb_model.safetensors\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"‚ùå Model weights not found: {model_path}\")\n",
    "    print(\"\\nüì• Please download the model weights first:\")\n",
    "    print('curl -L \"https://huggingface.co/clint-holt/AbLangPDB1/resolve/main/ablangpdb_model.safetensors?download=true\" -o ablangpdb_model.safetensors')\n",
    "else:\n",
    "    print(f\"‚úÖ Found model weights: {model_path}\")\n",
    "\n",
    "# Load model configuration and model\n",
    "print(\"\\nüîÑ Loading model...\")\n",
    "model_config = AbLangPairedConfig(checkpoint_filename=model_path)\n",
    "model = AbLangPaired(model_config, device)\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Single Antibody Example\n",
    "\n",
    "Generate an embedding for a single antibody sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example antibody sequences (SARS-CoV-2 neutralizing antibody)\n",
    "example_data = {\n",
    "    'HC_AA': [\"EVQLVESGGGLVQPGGSLRLSCAASGFNLYYYSIHWVRQAPGKGLEWVASISPYSSSTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCARGRWYRRALDYWGQGTLVTVSS\"],\n",
    "    'LC_AA': [\"DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQYPYYSSLITFGQGTKVEIK\"]\n",
    "}\n",
    "\n",
    "df_single = pd.DataFrame(example_data)\n",
    "print(\"Example antibody:\")\n",
    "print(f\"Heavy chain: {df_single['HC_AA'][0][:50]}...\")\n",
    "print(f\"Light chain: {df_single['LC_AA'][0][:50]}...\")\n",
    "print(f\"Heavy chain length: {len(df_single['HC_AA'][0])} amino acids\")\n",
    "print(f\"Light chain length: {len(df_single['LC_AA'][0])} amino acids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers\n",
    "print(\"Loading tokenizers for single sequence example...\")\n",
    "heavy_tokenizer = AutoTokenizer.from_pretrained(model_config.heavy_model_id, revision=model_config.heavy_revision)\n",
    "light_tokenizer = AutoTokenizer.from_pretrained(model_config.light_model_id, revision=model_config.light_revision)\n",
    "\n",
    "# Preprocess sequences (add spaces between amino acids)\n",
    "df_single[\"PREPARED_HC_SEQ\"] = df_single[\"HC_AA\"].apply(lambda x: \" \".join(list(x)))\n",
    "df_single[\"PREPARED_LC_SEQ\"] = df_single[\"LC_AA\"].apply(lambda x: \" \".join(list(x)))\n",
    "\n",
    "# Tokenize sequences\n",
    "h_tokens = heavy_tokenizer(df_single[\"PREPARED_HC_SEQ\"].tolist(), padding='longest', return_tensors=\"pt\")\n",
    "l_tokens = light_tokenizer(df_single[\"PREPARED_LC_SEQ\"].tolist(), padding='longest', return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Heavy chain tokens shape: {h_tokens['input_ids'].shape}\")\n",
    "print(f\"Light chain tokens shape: {l_tokens['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding\n",
    "print(\"Generating embedding for single antibody...\")\n",
    "with torch.no_grad():\n",
    "    embedding = model(\n",
    "        h_input_ids=h_tokens['input_ids'].to(device),\n",
    "        h_attention_mask=h_tokens['attention_mask'].to(device),\n",
    "        l_input_ids=l_tokens['input_ids'].to(device),\n",
    "        l_attention_mask=l_tokens['attention_mask'].to(device)\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Generated embedding shape: {embedding.shape}\")\n",
    "print(f\"üìä Embedding dimension: {embedding.shape[1]}\")\n",
    "print(f\"üî¢ First 5 embedding values: {embedding[0][:5].tolist()}\")\n",
    "print(f\"üìà Embedding norm: {torch.norm(embedding[0]).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Antibodies Example\n",
    "\n",
    "Process multiple antibody sequences efficiently using batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with multiple example antibodies\n",
    "multi_antibody_data = {\n",
    "    'HC_AA': [\n",
    "        # Example 1: SARS-CoV-2 antibody\n",
    "        \"EVQLVESGGGLVQPGGSLRLSCAASGFNLYYYSIHWVRQAPGKGLEWVASISPYSSSTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCARGRWYRRALDYWGQGTLVTVSS\",\n",
    "        # Example 2: Another antibody\n",
    "        \"QVQLVQSGAEVKKPGSSVKVSCKASGGTFSSYWIEWVRQAPGQGLEWMGIIYPILSEGSTKYYNEKFKDRATLSADTSTSTAYMELSSLTSEDTAVYYCARGGAYYGSGYYAMDYWGQGTLVTVSS\",\n",
    "        # Example 3: Third antibody\n",
    "        \"EVQLLESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCARYHGGDAMDYWGQGTLVTVSS\",\n",
    "        # Example 4: Fourth antibody\n",
    "        \"QVQLQQSGPGLVKPSQTLSLTCAISGDSVSSNSAAWNWIRQSPSRGLEWLGRTYYRSKWYNDYAVSVKSRITINPDTSKNQFSLQLNSVTPEDTAVYYCARYDILTGYCTNGVCYAMDYWGQGTLVTVSS\"\n",
    "    ],\n",
    "    'LC_AA': [\n",
    "        # Light chains corresponding to heavy chains above\n",
    "        \"DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQYPYYSSLITFGQGTKVEIK\",\n",
    "        \"EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPRLLIYGASSRATGIPDRFSGSGSGTDFTLTISRLEPEDFAVYYCQQYGSSPLTFGAGTKVEIK\",\n",
    "        \"DIQMTQSPSSLSASVGDRVTITCRASQSISSWLAWYQQKPGKAPKLLIYKASSLESGVPSRFSGSGSGTEFTLTISSLQPDDFATYYCQQYNSYSYTFGQGTKVEIK\",\n",
    "        \"DIVMTQTPKFLLVSAGDRVTITCRASQGISSALAWYQQKPGQAPRLLIYDASSRATGIPARFSGSGSGTDFTLTISRLEPEDFAVYYCQQFNSYPLTFGAGTKLELK\"\n",
    "    ],\n",
    "    'Antibody_ID': ['AB001', 'AB002', 'AB003', 'AB004'],\n",
    "    'Description': [\n",
    "        'SARS-CoV-2 neutralizing antibody',\n",
    "        'Example antibody 2', \n",
    "        'Example antibody 3',\n",
    "        'Example antibody 4'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_multi = pd.DataFrame(multi_antibody_data)\n",
    "print(f\"Created dataset with {len(df_multi)} antibodies:\")\n",
    "for i, row in df_multi.iterrows():\n",
    "    print(f\"  {row['Antibody_ID']}: {row['Description']} (HC: {len(row['HC_AA'])} aa, LC: {len(row['LC_AA'])} aa)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process multiple antibodies using batch processing\n",
    "batch_size = 2  # Small batch size for this example\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = tokenize_data(df_multi, model_config)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings_batch = embed_dataloader(dataloader, model, device)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated batch embeddings shape: {embeddings_batch.shape}\")\n",
    "print(f\"üìä Number of antibodies processed: {embeddings_batch.shape[0]}\")\n",
    "print(f\"üìè Embedding dimension: {embeddings_batch.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Embeddings\n",
    "\n",
    "Calculate similarities between antibody embeddings to see which ones might target similar epitopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise cosine similarities\n",
    "print(\"Calculating pairwise similarities...\")\n",
    "similarities = torch.cosine_similarity(embeddings_batch.unsqueeze(1), embeddings_batch.unsqueeze(0), dim=2)\n",
    "\n",
    "print(\"\\nüìä Cosine similarity matrix:\")\n",
    "print(\"    \", end=\"\")\n",
    "for j in range(len(df_multi)):\n",
    "    print(f\"  {df_multi.iloc[j]['Antibody_ID']:>6}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i in range(similarities.shape[0]):\n",
    "    print(f\"{df_multi.iloc[i]['Antibody_ID']:>6}: \", end=\"\")\n",
    "    for j in range(similarities.shape[1]):\n",
    "        print(f\"{similarities[i][j]:.3f}  \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Find most similar pairs (excluding self-similarity)\n",
    "print(\"\\nüîç Most similar antibody pairs:\")\n",
    "for i in range(len(df_multi)):\n",
    "    for j in range(i+1, len(df_multi)):\n",
    "        sim = similarities[i][j].item()\n",
    "        id1, id2 = df_multi.iloc[i]['Antibody_ID'], df_multi.iloc[j]['Antibody_ID']\n",
    "        print(f\"  {id1} ‚Üî {id2}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results\n",
    "\n",
    "Save the embeddings and results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to the dataframe\n",
    "df_multi['EMBEDDING'] = [emb.numpy() for emb in embeddings_batch]\n",
    "\n",
    "# Save to various formats\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Save as CSV (embeddings will be serialized)\n",
    "df_results = df_multi.copy()\n",
    "df_results['EMBEDDING_STR'] = df_results['EMBEDDING'].apply(lambda x: ','.join(map(str, x)))\n",
    "df_results.drop('EMBEDDING', axis=1).to_csv('antibody_results.csv', index=False)\n",
    "print(\"‚úÖ Saved results to antibody_results.csv\")\n",
    "\n",
    "# Save as pickle (preserves numpy arrays)\n",
    "df_multi.to_pickle('antibody_results.pkl')\n",
    "print(\"‚úÖ Saved results to antibody_results.pkl\")\n",
    "\n",
    "# Save similarity matrix\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarities.numpy(), \n",
    "    index=df_multi['Antibody_ID'], \n",
    "    columns=df_multi['Antibody_ID']\n",
    ")\n",
    "similarity_df.to_csv('antibody_similarities.csv')\n",
    "print(\"‚úÖ Saved similarity matrix to antibody_similarities.csv\")\n",
    "\n",
    "print(\"\\nüìÅ Generated files:\")\n",
    "print(\"  ‚Ä¢ antibody_results.csv - Antibody sequences and metadata\")\n",
    "print(\"  ‚Ä¢ antibody_results.pkl - Complete results with embeddings\")\n",
    "print(\"  ‚Ä¢ antibody_similarities.csv - Similarity matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Usage Tips\n",
    "\n",
    "### Performance Optimization\n",
    "- Use larger batch sizes (256-512) for better GPU utilization\n",
    "- Process sequences in similar length groups to minimize padding\n",
    "- Use `torch.cuda.empty_cache()` if running into memory issues\n",
    "\n",
    "### Interpretation\n",
    "- **High similarity (>0.8)**: Likely target similar epitopes\n",
    "- **Medium similarity (0.6-0.8)**: May share some epitope characteristics\n",
    "- **Low similarity (<0.6)**: Likely target different epitopes\n",
    "\n",
    "### Common Issues\n",
    "- **Sequence length**: AbLang works best with sequences <157 amino acids\n",
    "- **Stop codons**: Remove sequences containing '*' characters\n",
    "- **Memory**: Reduce batch size if encountering GPU memory errors\n",
    "\n",
    "### Next Steps\n",
    "- Explore the benchmarking suite in `/benchmarking/` directory\n",
    "- See the main README.md for more applications\n",
    "- Check out the paper for technical details: https://doi.org/10.1101/2025.02.25.640114"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}